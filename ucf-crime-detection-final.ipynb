{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2799594,"sourceType":"datasetVersion","datasetId":1710176}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"UCF Crime Detection Deep Learning Analysis","metadata":{}},{"cell_type":"markdown","source":"Gather data, determine the method of data collection and provenance of the data:\n\nI grabbed the UCF Crime Dataset from Kaggle.\n\nIdentify a Deep Learning Problem:\n\nI wanted to train a deep learning model to help solve a problem in the world. The problem I chose to focus on was crime. I imported the UCF Crime Dataset, a dataset that is split into a Test and Train folder each with label folders of Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Normal Videos, Road Accidents, Robbery, Shooting, Shoplifting, Stealing, and Vandalism. Each of these label folders contains a series of png images that are captured from video surveillance of the denoted crime or action occuring. ","metadata":{}},{"cell_type":"code","source":"# import necessary libraries\nimport os\nimport gc\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, ReLU)\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\nwarnings.filterwarnings(\"ignore\")\n\n# set paths for train and test data\npath_train = '../input/ucf-crime-dataset/Train'\npath_test = '../input/ucf-crime-dataset/Test'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-28T22:39:14.116786Z","iopub.status.idle":"2024-06-28T22:39:14.117181Z","shell.execute_reply.started":"2024-06-28T22:39:14.116986Z","shell.execute_reply":"2024-06-28T22:39:14.117002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to load and visualize images\ndef load_and_visualize_images(image_paths, target_size=(96, 96)):\n    fig, axes = plt.subplots(1, len(image_paths), figsize=(20, 20))\n    for img_path, ax in zip(image_paths, axes):\n        img = load_img(img_path, target_size=target_size)\n        ax.imshow(img)\n        ax.axis('off')\n    plt.show()\n    \n# example images for visualization\nexample_image_paths = [os.path.join(path_test, 'Vandalism', 'Vandalism007_x264_0.png'), os.path.join(path_test, 'Arrest', 'Arrest001_x264_0.png')]\nload_and_visualize_images(example_image_paths)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.119283Z","iopub.status.idle":"2024-06-28T22:39:14.119671Z","shell.execute_reply.started":"2024-06-28T22:39:14.119462Z","shell.execute_reply":"2024-06-28T22:39:14.119477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image data generator parameters\nimg_gen_params = {\n    \"rescale\": 1.0 / 255,\n    \"samplewise_center\": True,\n    \"samplewise_std_normalization\": True,\n    \"horizontal_flip\": True,\n    \"vertical_flip\": True\n}\n\n# create image data generator\nimg_gen = ImageDataGenerator(**img_gen_params)\nIMAGE_SHAPE = (96, 96, 3)\nbatch_size = 32\n\n# prepare train data generator\nimg_flow_train = img_gen.flow_from_directory(\n    path_train,\n    target_size=IMAGE_SHAPE[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\n# prepare test data generator\nimg_flow_test = img_gen.flow_from_directory(\n    path_test,\n    target_size=IMAGE_SHAPE[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.122374Z","iopub.status.idle":"2024-06-28T22:39:14.122811Z","shell.execute_reply.started":"2024-06-28T22:39:14.122583Z","shell.execute_reply":"2024-06-28T22:39:14.122599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize pixel intensity distribution\nsample_img = load_img(example_image_paths[0], target_size=IMAGE_SHAPE[:2])\nsample_img_array = img_to_array(sample_img)\nplt.figure(figsize=(10, 6))\nplt.hist(sample_img_array.flatten(), bins=50)\nplt.title('Pixel Intensity Distribution')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.124033Z","iopub.status.idle":"2024-06-28T22:39:14.124404Z","shell.execute_reply.started":"2024-06-28T22:39:14.124222Z","shell.execute_reply":"2024-06-28T22:39:14.124237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define cnn model\nkernel_size = (5, 5)\nfilters = (32, 64, 128)\ndrop_prob_conv = 0.3\ndrop_prob_dense = 0.3\nmodel = Sequential([\n    Conv2D(filters[0], kernel_size, padding=\"same\", kernel_initializer='he_normal', input_shape=IMAGE_SHAPE),\n    BatchNormalization(),\n    ReLU(),\n    Conv2D(filters[0], kernel_size, padding=\"same\", kernel_initializer='he_normal'),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(),\n    Dropout(drop_prob_conv),\n    Conv2D(filters[1], kernel_size, padding=\"same\", kernel_initializer='he_normal'),\n    BatchNormalization(),\n    ReLU(),\n    Conv2D(filters[1], kernel_size, padding=\"same\", kernel_initializer='he_normal'),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(),\n    Dropout(drop_prob_conv),\n    Conv2D(filters[2], kernel_size, padding=\"same\", kernel_initializer='he_normal'),\n    BatchNormalization(),\n    ReLU(),\n    Conv2D(filters[2], kernel_size, padding=\"same\", kernel_initializer='he_normal'),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(),\n    Dropout(drop_prob_conv),\n    Flatten(),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(drop_prob_dense),\n    Dense(256),\n    BatchNormalization(),\n    ReLU(),\n    Dropout(drop_prob_dense),\n    Dense(len(img_flow_train.class_indices), activation=\"softmax\")\n])\n\n# compile model\nmodel.compile(Adam(0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.125436Z","iopub.status.idle":"2024-06-28T22:39:14.125849Z","shell.execute_reply.started":"2024-06-28T22:39:14.125655Z","shell.execute_reply":"2024-06-28T22:39:14.125672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define callbacks\nearly_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=3, verbose=1, mode='max')\nlr_decay = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=1, min_lr=1e-5)\n# train model\nprint(\"Training the model...\")\nhistory = model.fit(\n    img_flow_train,\n    steps_per_epoch=img_flow_train.samples // batch_size,\n    epochs=1,\n    verbose=1,\n    validation_data=img_flow_test,\n    validation_steps=img_flow_test.samples // batch_size,\n    callbacks=[lr_decay, early_stopping]\n)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.128105Z","iopub.status.idle":"2024-06-28T22:39:14.128491Z","shell.execute_reply.started":"2024-06-28T22:39:14.128303Z","shell.execute_reply":"2024-06-28T22:39:14.128319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on test data\nval_steps = img_flow_test.samples // batch_size\ny_test_pred = model.predict(img_flow_test, steps=val_steps)\ny_test_true = img_flow_test.classes[:len(y_test_pred)]\n\n# calculate accuracy\naccuracy = np.equal(np.argmax(y_test_pred, axis=1), y_test_true).sum() / y_test_pred.shape[0]\nprint(f\"Test accuracy: {accuracy:.3f}.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.129729Z","iopub.status.idle":"2024-06-28T22:39:14.130253Z","shell.execute_reply.started":"2024-06-28T22:39:14.129988Z","shell.execute_reply":"2024-06-28T22:39:14.130009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot roc curve\nfpr, tpr, _ = roc_curve(y_test_true, y_test_pred[:, 1])\nauc_val = auc(fpr, tpr)\nprint(f\"Test AUC: {auc_val:.3f}.\")\nplt.figure()\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.plot(fpr, tpr, label=f\"ACC={accuracy:.4f}, AUC={auc_val:.4f}\")\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve\")\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.132348Z","iopub.status.idle":"2024-06-28T22:39:14.132891Z","shell.execute_reply.started":"2024-06-28T22:39:14.132590Z","shell.execute_reply":"2024-06-28T22:39:14.132631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot precision-recall curve\nprecision, recall, _ = precision_recall_curve(y_test_true, y_test_pred[:, 1])\naverage_precision = average_precision_score(y_test_true, y_test_pred[:, 1])\nprint(f\"Average precision score: {average_precision:.3f}.\")\nplt.figure()\nplt.step(recall, precision, where='post', label=f'AP={average_precision:.4f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.134905Z","iopub.status.idle":"2024-06-28T22:39:14.135407Z","shell.execute_reply.started":"2024-06-28T22:39:14.135146Z","shell.execute_reply":"2024-06-28T22:39:14.135167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# collect garbage\ngc.collect()\n\n# save predictions to csv\nfile_names = img_flow_test.filenames\ny_pred_df = pd.DataFrame({\"file_name\": file_names, \"label\": np.argmax(y_test_pred, axis=1)})\ny_pred_df.to_csv(\"y_pred.csv\", index=False)\nprint(y_pred_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:39:14.137384Z","iopub.status.idle":"2024-06-28T22:39:14.137922Z","shell.execute_reply.started":"2024-06-28T22:39:14.137645Z","shell.execute_reply":"2024-06-28T22:39:14.137667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion:\n\nThis project involved building a deep learning model to classify various types of crimes using the UCF Crime Dataset. \n\nI first imported necessary libraries and set up paths for training and test data, then loaded and visualized example images from the dataset to understand the data structure. I then used ImageDataGenerator for preprocessing the images, including rescaling, normalization, and augmentation techniques like flipping.\n\nFor the model architecture, I defined a Convolutional Neural Network (CNN) with multiple layers, including convolutional, batch normalization, ReLU activation, max-pooling, and dropout layers to prevent overfittingm then compiled the model using the Adam optimizer and categorical cross-entropy loss function.\n\nFor training, I made configured callbacks for early stopping and learning rate reduction based on validation accuracy, then trained the model on the training dataset and evaluated it using the validation dataset, which involved predicting the classes for the test dataset and calculating the accuracy, which was approximately 67.66%.\n\nNext I lotted the ROC curve and calculated the Area Under the Curve (AUC) to measure the model's performance, resulting in an AUC of approximately 0.503, and then plotted the precision-recall curve and calculated the average precision score, which was approximately 0.677.\n\nThe model achieved a test accuracy of around 67.66%, indicating a reasonable performance given the complexity of the task and the diversity of the dataset. The ROC and precision-recall curves provided insights into the model's performance across different thresholds.\n\nTo improve the model's performance, I am considering experimenting with deeper architectures, more advanced data augmentation techniques, and fine-tuning pre-trained models.\nAdditionally, I will be exploring different hyperparameters and training for more epochs.\nThe predictions were saved to a CSV file, providing a record of the predicted labels for the test dataset.","metadata":{}}]}